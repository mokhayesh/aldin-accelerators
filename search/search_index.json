{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Custom Built Solutions Aldin AI Accelerators <p>       Opinionated, reusable components for data &amp; AI delivery \u2014 designed to help teams ship reliable, safe and fast.     </p> <p> View Data Governance Explore Accelerators </p>"},{"location":"#explore-the-accelerators-accelerators","title":"Explore the accelerators { #accelerators }","text":"- :material-chat-processing: **Prompt Engineering**     Patterns for structured prompts, grounding, guardrails, and evaluation to make LLM features **reliable, safe, and repeatable**.     [:octicons-arrow-right-16: Open](prompt-engineering/index.md)  - :material-code-json: **Code Converter**     Oracle/PLSQL \u2192 Spark SQL utilities.     [:octicons-arrow-right-16: Open](code-converter/index.md)  - :material-account-cog: **Data Buddy**     Profiling, DQ, cataloging, anomalies, compliance, automation, conversational AI, knowledge files.     [:octicons-arrow-right-16: Open](data-buddy/index.md)  - :material-database-cog: **MDM**     Matching, survivorship, reference data flows.     [:octicons-arrow-right-16: Open](mdm/index.md)  - :material-puzzle: **Sidecar Apps**     Small UIs/services to extend your platform.     [:octicons-arrow-right-16: Open](sidecar-applications/index.md)  - :material-robot-excited: **Synthetic Data Generator**     Privacy-preserving test data at scale.     [:octicons-arrow-right-16: Open](synthetic-data-generator/index.md)"},{"location":"changelog/","title":"changelog","text":"<p>Placeholder page. Replace with real content.</p>"},{"location":"contributing/","title":"contributing","text":"<p>Placeholder page. Replace with real content.</p>"},{"location":"architecture/","title":"Data Architecture","text":"<p>Data Governance is the overarching framework that defines how data is owned, managed, accessed, protected, and measured. Data Architecture is the technical foundation that implements those policies in practice\u2014defining how platforms, integrations, and patterns are structured so the organization can deliver reusable, scalable, repeatable, and future-proof solutions.</p>   - :material-shield-crown: **Governance sets the rules**   - Ownership &amp; stewardship (RACI)   - Policies (privacy, retention, classification)   - SLAs/SLOs (quality, timeliness)   - Compliance &amp; audit evidence  - :material-domain: **Architecture makes them real**   - Platform &amp; storage layout (raw \u2192 curated \u2192 analytic)   - Integration patterns (batch, CDC, streaming, sidecar)   - Cross-cutting controls (security, lineage, observability)   - Interfaces &amp; contracts (schemas, APIs, data products)"},{"location":"architecture/#architecture-principles-for-great-governance","title":"Architecture principles for great governance","text":"<ul> <li>Policy-first design \u2014 technical choices must enforce classification, access, retention, and quality objectives.  </li> <li>Separation of concerns \u2014 ingest, process, serve, and govern are modular and independently scalable.  </li> <li>Standard patterns \u2014 a small set of approved blueprints (below) to speed delivery and reduce drift.  </li> <li>Product mindset \u2014 datasets are products with clear contracts, SLAs, and owners.  </li> <li>Evidence by default \u2014 every job emits lineage, metrics, and artifacts suitable for audit.  </li> <li>Cloud-agnostic where possible \u2014 abstractions (contracts, templates) outlive any one tool.</li> </ul>"},{"location":"architecture/#layers-responsibilities","title":"Layers &amp; responsibilities","text":"<ul> <li>Ingestion \u2013 file drops, APIs, streaming, and CDC.  </li> <li>Storage &amp; refinement \u2013 Raw/Bronze \u2192 Conformed/Silver \u2192 Curated/Gold \u2192 Analytic/Platinum.  </li> <li>Serving \u2013 SQL engines/semantic layers, BI, reverse ETL.  </li> <li>Cross-cutting \u2013 identity &amp; access, encryption, metadata catalog, lineage, data quality, monitoring, cost controls.</li> </ul> <p>Tip: make governance controls (classification, masking, DQ thresholds, retention) declarative and versioned so they can be applied consistently across layers.</p>"},{"location":"architecture/#standard-patterns-accelerators","title":"Standard patterns &amp; accelerators","text":"<p>Use these patterns to bake governance into delivery while staying fast:</p> <ul> <li> <p>Sidecar application \u2014 connect to an existing source, perform an action, and publish results without new pipelines. :octicons-arrow-right-16: Learn more</p> </li> <li> <p>Data Buddy \u2014 profiling, Data Quality rules, Cataloging, anomalies, compliance evidence, conversational AI. :octicons-arrow-right-16: Learn more</p> </li> <li> <p>MDM Hub \u2014 match/merge to Golden Records; redistribute to sources and curated zones. :octicons-arrow-right-16: Learn more</p> </li> <li> <p>Code Converter \u2014 modernize SQL logic consistently (e.g., PL/SQL \u2192 Spark SQL). :octicons-arrow-right-16: Learn more</p> </li> <li> <p>Synthetic Data Generator \u2014 privacy-preserving test data for repeatable pipelines and QA. :octicons-arrow-right-16: Learn more</p> </li> </ul>"},{"location":"architecture/#standards-interfaces","title":"Standards &amp; interfaces","text":"<ul> <li>Data product contract \u2013 name, domain, schema, semantics, SLAs, lineage, owners, access policy.  </li> <li>Schema &amp; API conventions \u2013 naming, versioning, deprecation, backward compatibility.  </li> <li>Security \u2013 classification \u2192 policy (masking, row filters), KMS encryption, least privilege.  </li> <li>Quality \u2013 rule packs per dimension (completeness, validity, uniqueness, timeliness, accuracy, consistency).  </li> <li>Observability \u2013 metrics (freshness, volume, schema change), logs, alerts, and SLOs.  </li> <li>CI/CD for data \u2013 tests, policy checks, and contract validation in every change.</li> </ul>"},{"location":"architecture/#reference-integrations-tool-agnostic","title":"Reference integrations (tool-agnostic)","text":"<ul> <li>Connectors: file/object, JDBC, message/stream, REST.  </li> <li>Processing: ELT/ETL engines, notebooks, batch &amp; streaming frameworks.  </li> <li>Catalog &amp; lineage: central metadata with APIs for write-back from jobs.  </li> <li>Orchestration: schedules, dependencies, approvals, and evidence bundle publishing.  </li> <li>Serving: query engines/semantic layers and BI tools with row/column-level security.</li> </ul>"},{"location":"architecture/#readiness-checklist","title":"Readiness checklist","text":"<ul> <li>Architecture document includes domain, data products, contracts, SLAs, and controls.  </li> <li>Lineage and DQ rule packs defined and stored with the product.  </li> <li>Access policies (classification \u2192 masking/filters) codified and tested.  </li> <li>Monitoring and alerts wired to SLAs/SLOs; cost guardrails in place.  </li> <li>Evidence (profiles, scorecards, catalogs, policies) exportable for audit.</li> </ul>"},{"location":"architecture/#where-to-go-next","title":"Where to go next","text":"<ul> <li> <p>See the Data Governance overview for principles and capability map. :octicons-arrow-right-16: Data Governance</p> </li> <li> <p>Browse the accelerators to apply these patterns quickly. :octicons-arrow-right-16: Explore accelerators</p> </li> </ul>"},{"location":"code-converter/","title":"code converter","text":"<p>Placeholder page. Replace with real content.</p>"},{"location":"data-buddy/","title":"Data Buddy","text":"<p>A collection of utilities that speed up day-to-day data work across profiling, quality, cataloging, anomalies, compliance, automation, conversational AI, and knowledge files.</p>"},{"location":"data-buddy/#quick-links","title":"Quick links","text":"<ul> <li>:material-chart-bar: Data Profiling</li> <li>:material-shield-check: Data Quality</li> <li>:material-book-open-variant: Data Cataloging</li> <li>:material-alert-decagram: Anomaly Detection</li> <li>:material-scale-balance: Compliance</li> <li>:material-cog-sync: Workflow Automation</li> <li>:material-robot-excited: Conversational AI</li> <li>:material-file-document: Knowledge Files</li> </ul>"},{"location":"data-buddy/#quickstart","title":"Quickstart","text":"<p>```bash</p>"},{"location":"data-buddy/#examples-replace-with-your-real-climodule-names-when-ready","title":"examples \u2013 replace with your real CLI/module names when ready","text":"<p>python -m databuddy --help python -m databuddy profile data/customers.parquet</p>"},{"location":"data-buddy/anomaly-detection/","title":"Anomaly detection","text":""},{"location":"data-buddy/anomaly-detection/#docsdata-buddyanomaly-detectionmd","title":"<code>docs/data-buddy/anomaly-detection.md</code>","text":"<p>```markdown</p>"},{"location":"data-buddy/anomaly-detection/#anomaly-detection","title":"Anomaly Detection","text":"<p>Detect unexpected shifts and outliers in metrics or distributions.</p>"},{"location":"data-buddy/anomaly-detection/#tasks","title":"Tasks","text":"<ul> <li>Z-score/IQR outliers</li> <li>Seasonal/level shifts</li> <li>Drift vs. baseline</li> </ul>"},{"location":"data-buddy/anomaly-detection/#example-placeholder","title":"Example (placeholder)","text":"<p>```bash python -m databuddy anomalies detect --metric dq_score --window 30 --method zscore</p>"},{"location":"data-buddy/compliance/","title":"Compliance","text":""},{"location":"data-buddy/compliance/#docsdata-buddycompliancemd","title":"<code>docs/data-buddy/compliance.md</code>","text":"<p>```markdown</p>"},{"location":"data-buddy/compliance/#compliance","title":"Compliance","text":"<p>Automate checks and evidence capture for policies and regulations (e.g., GDPR/CCPA/GLBA).</p>"},{"location":"data-buddy/compliance/#tasks","title":"Tasks","text":"<ul> <li>Control checks</li> <li>Evidence packaging</li> <li>Attestation reports</li> </ul>"},{"location":"data-buddy/compliance/#example-placeholder","title":"Example (placeholder)","text":"<p>```bash python -m databuddy compliance run --policy retention --output out/evidence.zip</p>"},{"location":"data-buddy/conversational-ai/","title":"Conversational ai","text":""},{"location":"data-buddy/conversational-ai/#docsdata-buddyconversational-aimd","title":"<code>docs/data-buddy/conversational-ai.md</code>","text":"<p>```markdown</p>"},{"location":"data-buddy/conversational-ai/#conversational-ai","title":"Conversational AI","text":"<p>Chat over datasets, rules, and docs; expose governance knowledge via Q&amp;A and copilots.</p>"},{"location":"data-buddy/conversational-ai/#tasks","title":"Tasks","text":"<ul> <li>RAG over catalog + policies</li> <li>Guided troubleshooting</li> <li>Red-teaming prompts</li> </ul>"},{"location":"data-buddy/conversational-ai/#example-placeholder","title":"Example (placeholder)","text":"<p>```bash python -m databuddy chat --kb ./knowledge --seed \"Show failed DQ rules last week\"</p>"},{"location":"data-buddy/data-cataloging/","title":"Data Cataloging","text":"<p>Build a living data catalog in minutes from any uploaded or connected dataset. Data Buddy uses AI to propose Business Terms, Descriptions, Definitions, Data Types, Patterns, Example Data, Privacy Classifications, and Encryption Requirements with consistent terminology and expert tone\u00e2\u20ac\u201dso you can scale metadata management, enable stewardship, and improve data quality &amp; compliance with defined and configurable SLAs.</p>   - :material-book-open-page-variant: **What it does**     Auto-generates field-level and table-level metadata (business + technical), applies privacy classifications, and suggests encryption requirements. Supports edit/approve workflows.  - :material-account-tie: **Why it matters**     Produces **consistent, steward-ready catalog entries** fast, accelerates onboarding, and makes downstream **DQ and compliance** measurable through SLAs.  - :material-database-arrow-right: **Typical inputs**     Connected tables (warehouse/lake) or uploaded files (CSV/Parquet).  - :material-file-chart: **Outputs**     Glossary terms, field dictionaries, classifications, SLA targets, and exportable evidence (CSV/JSON) for governance records."},{"location":"data-buddy/data-cataloging/#example-ui","title":"Example (UI)","text":"![](../_assets/catalog.png){ .screenshot }   AI-generated catalog entries from a sample dataset\u00e2\u20ac\u201dbusiness terms, descriptions, definitions, types, patterns, example data, privacy classification, and encryption requirement.    ![](../_assets/catalog-2.png){ .screenshot }   Configurable **SLA targets** (e.g., overall Quality, Completeness, Validity) are captured alongside the catalog to drive Data Quality and compliance reporting."},{"location":"data-buddy/data-cataloging/#quickstart","title":"Quickstart","text":"<p>=== \"UI\"     1. Upload a file or select a connected table.     2. Click Catalog \u00e2\u2020\u2019 Data Buddy infers the dictionary + classifications.     3. Switch to Edit to refine terms or add examples; set SLA targets.     4. Save / Export to CSV/JSON or sync to your enterprise catalog.</p> <p>=== \"CLI (placeholder)\"     ```bash     # Generate a catalog from a file     python -m databuddy catalog infer data/dataset.csv \\       --out out/catalog/catalog.csv</p> <pre><code># Apply/update SLA targets\npython -m databuddy catalog sla \\\n  --catalog out/catalog/catalog.csv \\\n  --sla \"quality=0.90,completeness=0.95,validity=0.90\" \\\n  --out out/catalog/catalog-with-sla.csv\n```\n</code></pre>"},{"location":"data-buddy/data-cataloging/#what-gets-captured","title":"What gets captured","text":"<ul> <li>Business Term &amp; Friendly Name </li> <li>Description &amp; Definition (business-grade wording)  </li> <li>Data Type &amp; Pattern (regex, format hints)  </li> <li>Example Data </li> <li>Data Privacy Classification (e.g., Public, Internal, Confidential, PII)  </li> <li>Encryption Requirement </li> <li>SLA Targets (Quality, Completeness, Validity \u00e2\u20ac\u201d configurable)</li> </ul> <p>Tip: keep short, action-oriented definitions; use example values that illustrate valid formats.</p>"},{"location":"data-buddy/data-cataloging/#governance-stewardship","title":"Governance &amp; stewardship","text":"<ul> <li>Assign Owners/Stewards (RACI) at domain or dataset level.  </li> <li>Track lineage to connect catalog \u00e2\u2020\u2019 DQ rules \u00e2\u2020\u2019 dashboards.  </li> <li>Use review queues for changes; export evidence bundles for audits.  </li> <li>Re-run inference when schemas evolve; differences become catalog PRs.</li> </ul>"},{"location":"data-buddy/data-cataloging/#suggested-next-steps","title":"Suggested next steps","text":"<ul> <li>Turn catalog fields into Data Quality rules and scorecards.  </li> <li>Surface definitions in Knowledge Files so conversational AI can answer using your catalog.  </li> <li>Schedule periodic profiling and anomaly detection to keep SLAs on track.</li> </ul>"},{"location":"data-buddy/data-profiling/","title":"Data Profiling","text":"<p>Get a fast understanding of a dataset\u00e2\u20ac\u2122s shape, quality, and risk before you build rules or dashboards.</p>   - :material-file-search: **What it does**     Schema &amp; type inference, basic statistics, distributions, uniqueness, null/blank counts, and outlier hints.  - :material-speedometer: **Why it matters**     Catches issues early (PII, empties, skew), guides **Data Quality** rule design, and speeds up onboarding.  - :material-database-arrow-right: **Typical inputs**     CSV, Parquet, or table references.  - :material-file-chart: **Outputs**     HTML/CSV reports, JSON summary, exports to evidence packages."},{"location":"data-buddy/data-profiling/#example-run-ui","title":"Example run (UI)","text":"![](../_assets/databuddy-profile-example.png){ .screenshot }   Data Buddy \u00e2\u20ac\u201d Profile analysis with completeness, uniqueness, min/max, median, and standard deviation per field."},{"location":"data-buddy/data-profiling/#quickstart","title":"Quickstart","text":"<p>=== \"UI\"     1. Upload a file or choose a table.     2. Click Profile \u00e2\u2020\u2019 Data Buddy computes schema, stats, and quality signals.     3. Export results as CSV/JSON and attach to your evidence pack.</p> <p>=== \"CLI (placeholder)\"     <code>bash     python -m databuddy profile data/synthetic_parts_10000.csv \\       --summary out/profile/synthetic_parts_10000.json \\       --report  out/profile/synthetic_parts_10000.csv</code></p>"},{"location":"data-buddy/data-profiling/#what-to-look-for","title":"What to look for","text":"<ul> <li>Completeness \u00e2\u20ac\u201c fields with low non-null ratios usually need rules or defaults.  </li> <li>Uniqueness \u00e2\u20ac\u201c candidate keys and dedupe hints.  </li> <li>Ranges &amp; distribution \u00e2\u20ac\u201c suspicious min/max, spikes, long tails \u00e2\u2020\u2019 check for unit or parsing issues.  </li> <li>Outliers \u00e2\u20ac\u201c values far from the median/stdev often indicate data entry or integration problems.  </li> <li>PII/PHI \u00e2\u20ac\u201c scan names/emails/IDs to drive Classification and access controls.</li> </ul>"},{"location":"data-buddy/data-profiling/#suggested-next-steps","title":"Suggested next steps","text":"<ul> <li>Turn findings into Data Quality rules (accuracy/completeness/timeliness/consistency/validity/uniqueness).  </li> <li>Capture business meaning and owners in Data Cataloging.  </li> <li>Enable Anomaly Detection on critical metrics.  </li> <li>Automate the profiling \u00e2\u2020\u2019 DQ \u00e2\u2020\u2019 report flow in Workflow Automation.</li> </ul>"},{"location":"data-buddy/data-profiling/#outputs-example-fields","title":"Outputs (example fields)","text":"Field Type Records Unique Completeness Min Max Median StdDev <code>part_number</code> string 10,000 10,000 100% \u00e2\u20ac\u201d \u00e2\u20ac\u201d \u00e2\u20ac\u201d \u00e2\u20ac\u201d <code>part_cost</code> decimal 10,000 9,798 100% 0.51 2,499.98 1,244.53 718.49 <code>number_of_units</code> int 10,000 4,295 100% 1 5,000 2,489.5 1,444.14 <p>!!! tip \"Export &amp; share\"     Include the CSV/JSON profile in your change ticket or governance evidence pack so reviewers can reproduce findings.</p>"},{"location":"data-buddy/data-quality/","title":"Data Quality","text":"<p>Ensure trusted data with configurable rules, scorecards, alerts, and a light stewardship workflow.</p>   - :material-shield-check: **What it does**     Evaluate data against rules mapped to standard dimensions (accuracy, completeness, timeliness, consistency, validity, uniqueness). Produce scores, exceptions, and evidence.  - :material-speedometer: **Why it matters**     Prevent downstream failures, protect analytics &amp; AI, and demonstrate compliance with measurable SLAs.  - :material-database-arrow-right: **Typical inputs**     Tables or files (CSV/Parquet), optional filters/partitions, rule packs.  - :material-file-chart: **Outputs**     Scorecards, exceptions, breach alerts, audit-ready evidence (CSV/JSON)."},{"location":"data-buddy/data-quality/#example-ui","title":"Example (UI)","text":"![](../_assets/databuddy-quality-example.png){ .screenshot }   Data Buddy \u00e2\u20ac\u201d Quality analysis with dimension scores, exceptions, and rules per field."},{"location":"data-buddy/data-quality/#quickstart","title":"Quickstart","text":"<p>=== \"UI\"     1. Select a table/file, then click Quality.     2. Pick a rule pack (or start with defaults per dimension).     3. Review scores &amp; exceptions \u00e2\u2020\u2019 export CSV/JSON for evidence.</p> <p>=== \"CLI (placeholder)\"     <code>bash     # Replace with your actual entry point when wired up     python -m databuddy dq run data/dataset.csv \\       --rules dq/rules.yml \\       --scorecard out/dq/scorecard.csv \\       --exceptions out/dq/exceptions.csv</code></p>"},{"location":"data-buddy/data-quality/#rule-packs-example","title":"Rule packs (example)","text":"<p>```yaml</p>"},{"location":"data-buddy/data-quality/#dqrulesyml","title":"dq/rules.yml","text":"<p>rules:   - name: email_valid     dimension: validity     expr: \"email ~ '^[^@]+@[^@]+\\.[^@]+$'\"     threshold: 0.99</p> <ul> <li> <p>name: phone_digits     dimension: consistency     expr: \"length(regexp_replace(phone, '[^0-9]', '')) BETWEEN 10 AND 15\"     threshold: 0.98</p> </li> <li> <p>name: address_not_blank     dimension: completeness     expr: \"address IS NOT NULL AND trim(address) &lt;&gt; ''\"     threshold: 0.999</p> </li> </ul>"},{"location":"data-buddy/knowledge-files/","title":"Knowledge Files","text":"<p>Turn your documents into an on-demand expert. Upload referenceable content (policies, playbooks, specs, spreadsheets, PDFs, markdown, etc.) and let a conversational AI answer questions, summarize, generate checklists, and point you to the exact source passages.</p>   - :material-book-open-page-variant: **What it does**     Indexes your docs and uses retrieval-augmented generation (RAG) so answers come **from your content**, not generic internet text.  - :material-account-tie: **Why it matters**     Gives teams **expert-level insight in seconds**, reduces context switching, and scales tribal knowledge.  - :material-file-upload: **Typical inputs**     Markdown, PDF, DOCX, XLSX/CSV, HTML, text notes, and curated exports from catalogs/BI.  - :material-shield-check: **Outputs**     Source-grounded answers, summaries, and snippets you can export to CSV/TXT for evidence."},{"location":"data-buddy/knowledge-files/#example-ui","title":"Example (UI)","text":"![](../_assets/knowledgefiles-part1.png){ .screenshot }   Load a knowledge pack, then **ask in natural language** (e.g., \u00e2\u20ac\u0153What are my vacation days?\u00e2\u20ac\u009d). The assistant answers from your files and can export the response."},{"location":"data-buddy/knowledge-files/#quickstart","title":"Quickstart","text":"<p>=== \"UI\"     1. Click Load Knowledge and select files or a folder.     2. Open Little Buddy and Ask a question (e.g., \u00e2\u20ac\u0153Summarize our retention policy\u00e2\u20ac\u009d).     3. Export the response (CSV/TXT) or paste into tickets and docs.</p> <p>=== \"CLI (placeholder)\"     ```bash     # Index a folder of documents     python -m databuddy kb index ./knowledge --db out/kb.index</p> <pre><code># Ask a question grounded in that knowledge base\npython -m databuddy chat --kb out/kb.index \\\n  --ask \"List our data quality SLAs and escalation path\"\n```\n</code></pre>"},{"location":"data-buddy/knowledge-files/#recommended-structure","title":"Recommended structure","text":"<p>```text knowledge/   glossary/   policies/   playbooks/   runbooks/   reference/</p>"},{"location":"data-buddy/workflow-automation/","title":"Workflow automation","text":""},{"location":"data-buddy/workflow-automation/#docsdata-buddyworkflow-automationmd","title":"<code>docs/data-buddy/workflow-automation.md</code>","text":"<p>```markdown</p>"},{"location":"data-buddy/workflow-automation/#workflow-automation","title":"Workflow Automation","text":"<p>Orchestrate tasks (profiling \u00e2\u2020\u2019 DQ \u00e2\u2020\u2019 anomalies \u00e2\u2020\u2019 reports) on a schedule or as CI jobs.</p>"},{"location":"data-buddy/workflow-automation/#tasks","title":"Tasks","text":"<ul> <li>DAG definitions</li> <li>Scheduled runs</li> <li>Notifications</li> </ul>"},{"location":"data-buddy/workflow-automation/#example-placeholder","title":"Example (placeholder)","text":"<p>```bash python -m databuddy flow run daily-governance</p>"},{"location":"data-governance/","title":"Data Governance","text":"<p>Data Governance aligns people, process, and technology so data is discoverable, trustworthy, compliant, and usable. Use this page as a quick brief for stakeholders and a map to the related accelerators.</p>"},{"location":"data-governance/#at-a-glance","title":"At a glance","text":"- :material-book-open-variant: **Metadata Management**     Build and maintain your catalog: capture technical + business metadata, lineage, and policies.  - :material-chart-bar: **Data Profiling**     Rapidly analyze datasets to surface structure, patterns, and outliers; inform rule design.  - :material-shield-check: **Data Quality**     Dimensions, rules, SLAs, monitoring, and scorecards to keep data reliable.  - :material-scale-balance: **Data Compliance**     Controls and policies by geography/domain (e.g., GDPR, CCPA, GLBA); automate evidence.  - :material-archive-lock: **Data Retention**     Lifecycle policies for archiving/disposing data; support \u00e2\u20ac\u0153right-to-erase\u00e2\u20ac\u009d requests.  - :material-account-multiple-check: **Master Data Management (MDM)**     Golden records for key entities; survivorship rules; improved cross-system consistency.  - :material-sitemap: **Data Architecture**     The technical blueprint\u00e2\u20ac\u201dzones, models, integrations (batch/stream/event), APIs, and IAM\u00e2\u20ac\u201dthat   implements governance principles and enables reusable, scalable delivery.  - :material-source-branch: **Data Lineage**     End-to-end traceability of data from source to consumption (technical + business lineage) for   impact analysis, troubleshooting, and auditability."},{"location":"data-governance/#capability-matrix","title":"Capability matrix","text":"| Capability / Offerings | What It Means | Who\u00e2\u20ac\u2122s Involved | Why It Matters | |---|---|---|---| | **Metadata Management** | Create, update, and maintain metadata (data about data); build &amp; operate the data catalog, including policies and lineage links. | Data Steward / Catalog Manager | Enables users to find &amp; understand data; drives self-service analytics. | | **Data Profiling** | Sample analysis to surface structure, patterns, outliers; full-dataset assessment across dimensions (completeness, accuracy, uniqueness, validity, timeliness); custom rule definition. | Data Analyst / Steward | Quick health check; guides deeper quality efforts. | | **Data Quality** | Dimensions, rules, thresholds/SLAs; continuous monitoring, exceptions, and scorecards. | Data Quality Lead / Engineer | Ensures analytics &amp; ML models use reliable data; reduces risk of bad decisions. | | **Data Compliance** | Policies &amp; controls by geography/domain (e.g., GDPR, CCPA, GLBA). Evidence collection and attestation workflows. | Legal &amp; Compliance Team | Avoids fines &amp; lawsuits; supports rights to know/share/port/delete. | | **Data Retention** | Lifecycle policies for archiving/disposing data; defensible deletion; \u00e2\u20ac\u0153right-to-erase\u00e2\u20ac\u009d requests. | Records Manager / IT | Meets legal mandates; simplifies audits and e-discovery. | | **Master Data Management (MDM)** | Centralize &amp; reconcile critical entity data (customers, products, suppliers). Golden record creation and survivorship rules. | MDM Lead / Architect | Single source of truth; improves cross-system consistency. | | **Data Architecture** | Technical blueprint: domain boundaries, data zones, canonical models, integration patterns (batch/stream/event), APIs, storage, compute, and IAM guardrails that implement governance. | Data / Platform Architect, Data Engineering Lead | Provides reusable patterns and platform guardrails for **scalable, secure, cost-effective** delivery; turns policy into practice. | | **Data Lineage** | End-to-end traceability of data (system, table, column). Technical + business lineage, change-impact analysis, and code-level drill-downs. | Data Engineer, Steward, Audit / Compliance | Builds trust and speeds issue resolution; supports audits and safe change management. |   <p>Looking to evaluate maturity? See Assessment Questions.</p>"},{"location":"data-governance/assessment/","title":"Assessment Questions","text":"<p>Use these questions in discovery workshops to baseline governance maturity. Capture evidence/links and score 0\u00e2\u20ac\u201c3 for each area.</p>"},{"location":"data-governance/assessment/#quick-score-rubric","title":"Quick-score rubric","text":"Score Description 0 Not in place 1 Ad hoc / informal 2 Defined / documented 3 Managed / automated &amp; measured"},{"location":"data-governance/assessment/#the-questions","title":"The questions","text":"<p>??? info \"1) Metadata enrichment\"     Ask: How is metadata enriched today (classifications, sensitivity/privacy tags, policies)? Who owns and maintains it? Evidence: catalog enrichments; stewards by domain; update cadence; policy mapping. Score: 0=no enrichment \u00e2\u20ac\u00a2 1=manual/spotty \u00e2\u20ac\u00a2 2=stewarded &amp; repeatable \u00e2\u20ac\u00a2 3=policy-driven &amp; automated.</p> <p>??? info \"2) Source prioritization &amp; CDEs\"     Ask: Do you have a prioritized inventory of data sources and defined Critical Data Elements (CDEs)? What criteria are used? Evidence: source list; ranking criteria (risk, value, usage); approved CDE list per domain. Score: 0=none \u00e2\u20ac\u00a2 1=informal \u00e2\u20ac\u00a2 2=defined + criteria \u00e2\u20ac\u00a2 3=governed with periodic review.</p> <p>??? info \"3) Knowledge capture\"     Ask: Where is tacit/tribal knowledge documented today, and should it be formalized in the data catalog? Evidence: links to Confluence/Notion/SharePoint; business definitions; gaps. Score: 0=nowhere \u00e2\u20ac\u00a2 1=wiki scattered \u00e2\u20ac\u00a2 2=catalog integrated \u00e2\u20ac\u00a2 3=workflow to curate &amp; approve.</p> <p>??? info \"4) Lineage\"     Ask: How is lineage captured (technical &amp; business)? What\u00e2\u20ac\u2122s the coverage, depth, and refresh cadence? Evidence: tool outputs; % coverage; refresh schedule; impact analysis examples. Score: 0=none \u00e2\u20ac\u00a2 1=partial/manual \u00e2\u20ac\u00a2 2=tool-based with coverage targets \u00e2\u20ac\u00a2 3=automated w/ CI/CD harvesting.</p> <p>??? info \"5) Data quality\"     Ask: Are DQ rules configurable and mapped to dimensions (accuracy, completeness, timeliness, consistency, validity, uniqueness)? How are thresholds, exceptions, and monitoring handled? Evidence: rule repo; SLA thresholds; exception workflow; alerts/dashboards. Score: 0=none \u00e2\u20ac\u00a2 1=ad hoc \u00e2\u20ac\u00a2 2=standardized dimensions &amp; SLAs \u00e2\u20ac\u00a2 3=continuous monitoring + breach escalation.</p> <p>??? info \"6) Stewardship &amp; ownership\"     Ask: Are data owners/stewards defined and assigned with clear RACI and capacity to act? Evidence: steward registry; RACI by domain/product; time allocation. Score: 0=unknown \u00e2\u20ac\u00a2 1=names only \u00e2\u20ac\u00a2 2=RACI formalized \u00e2\u20ac\u00a2 3=accountabilities measured (SLOs).</p> <p>??? info \"7) Governance operating model\"     Ask: Are councils/committees, roles/responsibilities, decision rights, and change processes established? How are policies approved and enforced? Evidence: charters; decision logs; policy lifecycle; enforcement tooling. Score: 0=none \u00e2\u20ac\u00a2 1=informal \u00e2\u20ac\u00a2 2=documented &amp; running \u00e2\u20ac\u00a2 3=audited with KPIs.</p> <p>??? info \"8) AI readiness\"     Ask: What is the organization\u00e2\u20ac\u2122s AI readiness/maturity (data availability, governance controls, risk/compliance guardrails, model oversight)? Evidence: approved datasets for AI; PII handling; usage policies; model review gates; red-teaming. Score: 0=unknown \u00e2\u20ac\u00a2 1=pilot-only guardrails \u00e2\u20ac\u00a2 2=enterprise controls \u00e2\u20ac\u00a2 3=governed lifecycle (data\u00e2\u2020\u2019model).</p> <p>??? info \"9) Compliance &amp; risk\"     Ask: Which regulations and internal policies drive governance requirements? How are controls evidenced and audited? Evidence: control library mapped to regs; evidence collection; audit cadence; exceptions register. Score: 0=unmapped \u00e2\u20ac\u00a2 1=manual audits \u00e2\u20ac\u00a2 2=mapped with evidence \u00e2\u20ac\u00a2 3=continuous assurance &amp; attestations.</p> <p>??? info \"10) Metrics &amp; value\"     Ask: What KPIs are tracked (policy coverage, DQ scores, issue MTTR, adoption)? How is value realized and reported? Evidence: KPI list; dashboards; executive reporting; value stories. Score: 0=none \u00e2\u20ac\u00a2 1=lagging only \u00e2\u20ac\u00a2 2=standard KPI set \u00e2\u20ac\u00a2 3=targets with actions &amp; ROI.</p> <p>??? info \"11) Access &amp; security\"     Ask: How are classification-driven access controls, segregation of duties, and retention/disposition managed? Evidence: RBAC/ABAC; masking/row-level security; SoD rules; retention schedules. Score: 0=local rules \u00e2\u20ac\u00a2 1=policy but uneven \u00e2\u20ac\u00a2 2=centralized enforcement \u00e2\u20ac\u00a2 3=auto-provisioning + recert.</p> <p>??? info \"12) Enablement &amp; adoption\"     Ask: How are stakeholders trained and supported (playbooks, office hours, communities of practice)? How is feedback incorporated? Evidence: training plans; playbooks/templates; engagement metrics; feedback backlog \u00e2\u2020\u2019 policy updates. Score: 0=none \u00e2\u20ac\u00a2 1=one-off sessions \u00e2\u20ac\u00a2 2=programmatic enablement \u00e2\u20ac\u00a2 3=measured adoption &amp; improvement.</p>"},{"location":"data-governance/assessment/#quick-worksheet-copypaste","title":"Quick worksheet (copy/paste)","text":"<ul> <li>[ ] 1. Metadata enrichment \u00e2\u20ac\u201d Score: <code>0/1/2/3</code> \u00e2\u20ac\u201d Notes:  </li> <li>[ ] 2. Source prioritization &amp; CDEs \u00e2\u20ac\u201d Score: <code>0/1/2/3</code> \u00e2\u20ac\u201d Notes:  </li> <li>[ ] 3. Knowledge capture \u00e2\u20ac\u201d Score: <code>0/1/2/3</code> \u00e2\u20ac\u201d Notes:  </li> <li>[ ] 4. Lineage \u00e2\u20ac\u201d Score: <code>0/1/2/3</code> \u00e2\u20ac\u201d Notes:  </li> <li>[ ] 5. Data quality \u00e2\u20ac\u201d Score: <code>0/1/2/3</code> \u00e2\u20ac\u201d Notes:  </li> <li>[ ] 6. Stewardship &amp; ownership \u00e2\u20ac\u201d Score: <code>0/1/2/3</code> \u00e2\u20ac\u201d Notes:  </li> <li>[ ] 7. Operating model \u00e2\u20ac\u201d Score: <code>0/1/2/3</code> \u00e2\u20ac\u201d Notes:  </li> <li>[ ] 8. AI readiness \u00e2\u20ac\u201d Score: <code>0/1/2/3</code> \u00e2\u20ac\u201d Notes:  </li> <li>[ ] 9. Compliance &amp; risk \u00e2\u20ac\u201d Score: <code>0/1/2/3</code> \u00e2\u20ac\u201d Notes:  </li> <li>[ ] 10. Metrics &amp; value \u00e2\u20ac\u201d Score: <code>0/1/2/3</code> \u00e2\u20ac\u201d Notes:  </li> <li>[ ] 11. Access &amp; security \u00e2\u20ac\u201d Score: <code>0/1/2/3</code> \u00e2\u20ac\u201d Notes:  </li> <li>[ ] 12. Enablement &amp; adoption \u00e2\u20ac\u201d Score: <code>0/1/2/3</code> \u00e2\u20ac\u201d Notes:  </li> </ul> <p>!!! tip \"Jump to accelerators\"     - Architecture \u00e2\u2020\u2019 ../architecture/index.md     - Data Buddy \u00e2\u2020\u2019 ../data-buddy/index.md     - Synthetic Data Generator \u00e2\u2020\u2019 ../synthetic-data-generator/index.md     - MDM \u00e2\u2020\u2019 ../mdm/index.md</p>"},{"location":"mdm/","title":"Master Data Management (MDM) Hub","text":"<p>A sidecar-style MDM hub connects to a data source that aggregates multiple third-party vendor systems (ERPs), pulls the records into the hub, matches &amp; merges them to produce a Golden Record, and then redistributes those Golden Records back to the sources for reconciliation and to downstream destinations (e.g., curated/reporting layers) for analytics and accounting\u2014without standing up a heavy framework, pipeline, or config files.</p>"},{"location":"mdm/#why-an-mdm-hub","title":"Why an MDM hub?","text":"<ul> <li>:material-account-box-multiple: Single source of truth \u2014 consistent customer/product/vendor entities across ERPs.</li> <li>:material-swap-horizontal-bold: Operational feedback \u2014 push Golden Records back to originating ERPs to correct or reconcile.</li> <li>:material-shield-check: Governed &amp; auditable \u2014 stewardship workflow, lineage, and evidence for audits.</li> <li>:material-speedometer: Fast to adopt \u2014 run as a sidecar app; no changes to existing ETL.</li> </ul>"},{"location":"mdm/#high-level-architecture","title":"High-level architecture","text":"![](../_assets/mdm-architecture.png){ .screenshot }   Possible future state: data lands in the lake, MDM hub performs match-merge to create Golden Records, pushes results back to ERPs for reconciliation, and publishes curated assets for BI (Athena/QuickSight/Power BI)."},{"location":"mdm/#how-it-works-typical-flow","title":"How it works (typical flow)","text":"<ol> <li>Land ERP data into the lake/landing zone (e.g., S3).  </li> <li>Ingest into the MDM hub (sidecar app) \u2014 pull snapshots or increments.  </li> <li>Standardize &amp; cleanse (trim, casing, normalize codes, reference data lookups).  </li> <li>Match &amp; merge \u2014 identity resolution + survivorship rules to create the Golden Record.  </li> <li>Redistribute:  </li> <li>Back to sources (ERPs) for reconciliation/correction.  </li> <li>To curated/reporting layers for analytics, accounting, and downstream apps.  </li> <li>Catalog &amp; lineage entries are created/updated; DQ and policy checks run; alerts raise on exceptions.</li> </ol>"},{"location":"mdm/#common-actions","title":"Common actions","text":"<ul> <li>Standardization, parsing, and enrichment  </li> <li>Deterministic + probabilistic matching </li> <li>Survivorship (source priority, timeliness, field-level confidence)  </li> <li>Reference data sync (codes, hierarchies)  </li> <li>Stewardship queue for ambiguous matches  </li> <li>Change data publishing (delta feeds, events)</li> </ul>"},{"location":"mdm/#implementation-notes","title":"Implementation notes","text":"<ul> <li>Inputs: tables/files from multiple ERPs; optional CDC/incremental extracts; reference datasets.  </li> <li>Outputs: Golden Records table; change log/events; exception buckets; curated analytic assets.  </li> <li>Security: PII handling, row/column masking, audit logs, SoD checks.  </li> <li>Ops: scheduled or event-driven; idempotent processing; health &amp; value KPIs (match rate, steward MTTR, golden coverage).  </li> </ul>"},{"location":"mdm/#related-accelerators","title":"Related accelerators","text":"<ul> <li>:material-account-cog: Data Buddy \u2014 profiling, DQ, cataloging, anomalies  </li> <li>:material-puzzle: Sidecar Applications \u2014 architecture pattern used by the hub  </li> <li>:material-robot-excited: Synthetic Data Generator \u2014 test data for match/merge tuning</li> </ul>"},{"location":"prompt-engineering/","title":"Prompt Engineering","text":"<p>Design prompts and context pipelines that are accurate, safe, and testable\u00e2\u20ac\u201dso conversational and generative features behave like products, not demos.</p>"},{"location":"prompt-engineering/#what-it-covers","title":"What it covers","text":"<ul> <li>Structured prompts &amp; templates \u00e2\u20ac\u201c roles, constraints, step-by-step formats.</li> <li>Grounding &amp; retrieval (RAG) \u00e2\u20ac\u201c cite sources, control drift, reduce hallucination.</li> <li>Guardrails \u00e2\u20ac\u201c instruction hierarchy, input/output validation, red-team prompts.</li> <li>Evaluation \u00e2\u20ac\u201c golden sets, rubric scoring, regression tests in CI.</li> <li>Observability \u00e2\u20ac\u201c prompt/version tracking, cost/latency metrics, feedback loops.</li> </ul>"},{"location":"prompt-engineering/#quickstart-placeholders","title":"Quickstart (placeholders)","text":"<p>```bash</p>"},{"location":"prompt-engineering/#run-an-eval-suite-against-a-prompt-template-golden-set","title":"Run an eval suite against a prompt template + golden set","text":"<p>python -m prompts eval \\   --template prompts/qa.jinja \\   --dataset evals/governance_qa.jsonl \\   --metrics exact_match,bleu,judge</p>"},{"location":"prompt-engineering/#batch-generate-with-retrieval-and-citations","title":"Batch-generate with retrieval and citations","text":"<p>python -m prompts generate \\   --template prompts/rag_cited.jinja \\   --kb ./knowledge \\   --questions data/questions.csv \\   --out out/answers.csv</p>"},{"location":"sidecar-applications/","title":"Sidecar Applications","text":"<p>A sidecar application connects to an existing data source (data lake / warehouse / data mart), collects data, performs an action, and redistributes the results to another location or layer \u2014 without needing a heavy framework, pipeline, or config files.</p>"},{"location":"sidecar-applications/#why-sidecar","title":"Why sidecar?","text":"<ul> <li>:material-flash: Fast to stand up \u2014 point to a source, run an action, publish results.</li> <li>:material-swap-horizontal: Decoupled \u2014 doesn\u2019t change the source system or require new ETL.</li> <li>:material-shield-check: Governable \u2014 results can be cataloged, quality-checked, and monitored.</li> <li>:material-docker: Portable \u2014 run locally, in a container, or as a scheduled job.</li> </ul>"},{"location":"sidecar-applications/#high-level-architecture","title":"High-level architecture","text":"![](../_assets/sidecar-architecture.png){ .screenshot }   Sidecar Applications / Accelerators \u2014 read from the production lake, perform an action, push curated results, catalog, and query."},{"location":"sidecar-applications/#how-it-works-typical-flow","title":"How it works (typical flow)","text":"<ol> <li>Source loads data to the production lake / warehouse.</li> <li>Sidecar connects and collects a working set (query, sample, partition).</li> <li>Action performed (see examples below).</li> <li>Results are written to a curated / golden layer or another target.</li> <li>Catalog &amp; lineage updated; results are queryable for BI/analytics and monitored by governance tools.</li> </ol>"},{"location":"sidecar-applications/#common-actions","title":"Common actions","text":"<ul> <li>Data Profiling \u00b7 Data Quality \u00b7 Data Cataloging \u00b7 Anomaly Detection </li> <li>Compliance \u00b7 Workflow Automation \u00b7 Conversational AI / RAG \u00b7 Knowledge Files</li> </ul>"},{"location":"sidecar-applications/#implementation-notes","title":"Implementation notes","text":"<ul> <li>Inputs: table or file path(s) (CSV/Parquet), optional filters/partitions.  </li> <li>Outputs: curated tables/files, metrics, logs/evidence; registered in the catalog.  </li> <li>Security: least-privilege creds; classification-driven access on outputs.  </li> <li>Ops: schedule with a job runner or CI; emit metrics for health and value tracking.</li> </ul>"},{"location":"synthetic-data-generator/","title":"synthetic data generator","text":"<p>Placeholder page. Replace with real content.</p>"},{"location":"test-data/","title":"test data","text":"<p>Placeholder page. Replace with real content.</p>"}]}